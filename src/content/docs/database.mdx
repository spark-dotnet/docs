---
title: "Database"
description: "Spark database"
---

## Introduction
Spark sets up your database connections for you. The only thing you have to worry about is setting up your configurations for which database type to use and its connection details.

Spark Supported database types:
- SQLite
- PostgreSQL
- MySQL
- SqlServer

## Configuration
By default, when a new Spark app is created, it comes with a SQLite already wired up. Databases configurations are setup in your `.env` file.

You can switch your database type by changing the `DB_CONNECTION` value to one of our other supported databases.

### SQlite ENV Example
```ini
DB_CONNECTION=sqlite
DB_HOST=null
DB_PORT=null
DB_DATABASE=Spark.db
DB_USERNAME=root
DB_PASSWORD=
```

### PostgreSQL ENV Example
```ini
DB_CONNECTION=postgres
DB_HOST=localhost
DB_PORT=5432
DB_DATABASE=your_db_name
DB_USERNAME=your_user
DB_PASSWORD=your_user_password
```
### MySQL ENV Example
```ini
DB_CONNECTION=mysql
DB_HOST=localhost
DB_PORT=3306
DB_DATABASE=your_db_name
DB_USERNAME=your_user
DB_PASSWORD=your_user_password
```
### SqlServer ENV Example
```ini
DB_CONNECTION=sqlserver
DB_HOST=localhost
# if using sqlexpress locally, this is what your DB_HOST will look like
# DB_HOST=localhost\SQLEXPRESS 
DB_PORT=1433
DB_DATABASE=your_db_name
DB_USERNAME=your_user
DB_PASSWORD=

# SQL Server Only
DB_TRUST_CERTIFICATE=true
DB_INTEGRATED_SECURITY=true
```
If you set `DB_INTEGRATED_SECURITY` to true, SQL Server will use your windows login to connect to the DB. This is great for local developement. If it is false, use a dedicated SQL Server user.

## EF Core
Spark utilizes **EF Core** to make interacting with your database as simple as possible. If you've never used EF Core before, you can check out Microsofts documentation [here](https://learn.microsoft.com/en-us/ef/core/).

To learn more about how Spark utilizes **EF Core migrations** go [here](/docs/models-and-migrations).

To learn more about how Spark utilizes **EF Core data access** go [here](/docs/data-access).

## Database Context
When a Spark app starts, a DB Context Factory is created and added to your apps DI container via Sparks internal service registration.

> Spark uses a DB Context Factory because Blazor applications do not create a service scope that aligned with the desired DB Context lifetime and will result in errors in your Blazor app.
> You can read more about this [here](https://learn.microsoft.com/en-us/ef/core/dbcontext-configuration/#using-a-dbcontext-factory-eg-for-blazor).

You can inject the DB Context Factory in your services like this:

```csharp
public class UsersService
{
    private readonly IDbContextFactory<DatabaseContext> _factory;

    public UsersService(IDbContextFactory<DatabaseContext> factory)
    {
        _factory = factory;
    }
}
```

And in your Razor components or pages like this:
```razor
@page "/"
@inject IDbContextFactory<DatabaseContext> factory
<section>
    <h1>
        Some page
    </h1>
</section>
@code {
    protected override async Task OnInitializedAsync()
    {
        using var db = factory.CreateDbContext();
        // query some data with db context
    }
}
```